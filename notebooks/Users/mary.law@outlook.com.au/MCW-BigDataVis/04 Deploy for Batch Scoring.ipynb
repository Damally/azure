{"cells":[{"cell_type":"markdown","source":["For the batch scoring, we will persist the values in a new global persistent Databricks table. In production data workloads, you may save the scored data to Blob Storage, Azure Cosmos DB, or other serving layer. Another implementation detail we are skipping for the lab is processing only new files. This can be accomplished by creating a widget in the notebook that accepts a path parameter that is passed in from Azure Data Factory."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Bucketizer\nfrom pyspark.sql.functions import array, col, lit\nfrom pyspark.sql.types import *"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["Replace STORAGE-ACCOUNT-NAME with the name of your storage account. You can find this in the Azure portal by locating the storage account that you created in the lab setup, within your resource group. The container name is set to the default used for this lab. If yours is different, update the containerName variable accordingly."],"metadata":{}},{"cell_type":"code","source":["accountName = \"mlbricksdatastore\"\ncontainerName = \"sparkcontainer\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["Define the schema for the CSV files"],"metadata":{}},{"cell_type":"code","source":["data_schema = StructType([\n        StructField('OriginAirportCode',StringType()),\n        StructField('Month', IntegerType()),\n        StructField('DayofMonth', IntegerType()),\n        StructField('CRSDepHour', IntegerType()),\n        StructField('DayOfWeek', IntegerType()),\n        StructField('Carrier', StringType()),\n        StructField('DestAirportCode', StringType()),\n        StructField('DepDel15', IntegerType()),\n        StructField('WindSpeed', DoubleType()),\n        StructField('SeaLevelPressure', DoubleType()),  \n        StructField('HourlyPrecip', DoubleType())])"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Create a new DataFrame from the CSV files, applying the schema"],"metadata":{}},{"cell_type":"code","source":["dfDelays = spark.read.csv(\"wasbs://\" + containerName + \"@\" + accountName + \".blob.core.windows.net/FlightsAndWeather/*/*/FlightsAndWeather.csv\",\n                    schema=data_schema,\n                    sep=\",\",\n                    header=True)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Load the trained machine learning model you created earlier in the lab"],"metadata":{}},{"cell_type":"code","source":["# Load the saved pipeline model\nmodel = PipelineModel.load(\"/flightDelayModel\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Make a prediction against the loaded data set"],"metadata":{}},{"cell_type":"code","source":["# Make a prediction against the dataset\nprediction = model.transform(dfDelays)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Save the scored data into a new global table called **scoredflights**"],"metadata":{}},{"cell_type":"code","source":["prediction.write.mode(\"overwrite\").saveAsTable(\"scoredflights\")"],"metadata":{},"outputs":[],"execution_count":14}],"metadata":{"name":"04 Deploy for Batch Scoring","notebookId":445132884519752},"nbformat":4,"nbformat_minor":0}