{"cells":[{"cell_type":"markdown","source":["For the batch scoring, we will persist the values in a new global persistent Databricks table. In production data workloads, you may save the scored data to Blob Storage, Azure Cosmos DB, or other serving layer. Another implementation detail we are skipping for the lab is processing only new files. This can be accomplished by creating a widget in the notebook that accepts a path parameter that is passed in from Azure Data Factory."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Bucketizer\nfrom pyspark.sql.functions import array, col, lit\nfrom pyspark.sql.types import *"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["Replace STORAGE-ACCOUNT-NAME with the name of your storage account. You can find this in the Azure portal by locating the storage account that you created in the lab setup, within your resource group. The container name is set to the default used for this lab. If yours is different, update the containerName variable accordingly."],"metadata":{}},{"cell_type":"code","source":["accountName = \"mlbricksdatastore\"\ncontainerName = \"sparkcontainer\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# mounted the storage account so no need to do it again\n#dbutils.fs.mount(\n #source = \"wasbs://sparkcontainer@mlbricksdatastore.blob.core.windows.net/\",\n  # mount_point = \"/mnt/mcw/\",\n   #extra_configs = {\"fs.azure.sas.source.mlbrickstore.blob.core.windows.net\": \"?sv=sv=2018-03-28&ss=b&srt=sco&sp=rwdlac&se=2021-01-07T08:37:52Z&st=2019-01-07T00:37:52Z&spr=https&sig=3vforJjVn840tqhhydmu9XNdx8l6KC%2B2LJbT0zCEbHw%3D\"})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-10036749142644&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      3</span>  source <span class=\"ansiyellow\">=</span> <span class=\"ansiblue\">&quot;wasbs://sparkcontainer@mlbricksdatastore.blob.core.windows.net/&quot;</span><span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span>    mount_point <span class=\"ansiyellow\">=</span> <span class=\"ansiblue\">&quot;/mnt/mcw/&quot;</span><span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 5</span><span class=\"ansiyellow\">    extra_configs = {&quot;fs.azure.sas.source.mlbrickstore.blob.core.windows.net&quot;: &quot;?sv=sv=2018-03-28&amp;ss=b&amp;srt=sco&amp;sp=rwdlac&amp;se=2021-01-07T08:37:52Z&amp;st=2019-01-07T00:37:52Z&amp;spr=https&amp;sig=3vforJjVn840tqhhydmu9XNdx8l6KC%2B2LJbT0zCEbHw%3D&quot;})\n</span>\n<span class=\"ansigreen\">/local_disk0/tmp/1549159416164-0/dbutils.py</span> in <span class=\"ansicyan\">f_with_exception_handling</span><span class=\"ansiblue\">(*args, **kwargs)</span>\n<span class=\"ansigreen\">    278</span>                     exc<span class=\"ansiyellow\">.</span>__context__ <span class=\"ansiyellow\">=</span> <span class=\"ansigreen\">None</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    279</span>                     exc<span class=\"ansiyellow\">.</span>__cause__ <span class=\"ansiyellow\">=</span> <span class=\"ansigreen\">None</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 280</span><span class=\"ansiyellow\">                     </span><span class=\"ansigreen\">raise</span> exc<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    281</span>             <span class=\"ansigreen\">return</span> f_with_exception_handling<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    282</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">ExecutionError</span>: An error occurred while calling o262.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: Could not retrieve either the Azure storage account access key through option key fs.azure.account.key.mlbricksdatastore.blob.core.windows.net or the SAS token for the Azure container through option key fs.azure.sas.sparkcontainer.mlbricksdatastore.blob.core.windows.net.; nested exception is: \n\tjava.lang.IllegalArgumentException: Could not retrieve either the Azure storage account access key through option key fs.azure.account.key.mlbricksdatastore.blob.core.windows.net or the SAS token for the Azure container through option key fs.azure.sas.sparkcontainer.mlbricksdatastore.blob.core.windows.net.\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:54)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:451)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: Could not retrieve either the Azure storage account access key through option key fs.azure.account.key.mlbricksdatastore.blob.core.windows.net or the SAS token for the Azure container through option key fs.azure.sas.sparkcontainer.mlbricksdatastore.blob.core.windows.net.\n\tat com.databricks.backend.daemon.data.server.backend.FileSystemBackendManager.$anonfun$getAzureStorageCredentials$4(FileSystemBackendManager.scala:419)\n\tat scala.Option.getOrElse(Option.scala:138)\n\tat com.databricks.backend.daemon.data.server.backend.FileSystemBackendManager.getAzureStorageCredentials(FileSystemBackendManager.scala:415)\n\tat com.databricks.backend.daemon.data.server.backend.FileSystemBackendManager.resolveCredentials(FileSystemBackendManager.scala:182)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:43)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:83)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:82)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:82)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:272)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:252)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:42)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:58)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:58)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:38)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$2(UsageLogging.scala:359)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:235)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:227)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:268)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:264)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:345)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:320)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:38)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:346)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:346)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:269)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:180)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:235)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:227)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:106)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:268)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:264)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:106)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:171)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:133)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["Define the schema for the CSV files"],"metadata":{}},{"cell_type":"code","source":["data_schema = StructType([\n        StructField('OriginAirportCode',StringType()),\n        StructField('Month', IntegerType()),\n        StructField('DayofMonth', IntegerType()),\n        StructField('CRSDepHour', IntegerType()),\n        StructField('DayOfWeek', IntegerType()),\n        StructField('Carrier', StringType()),\n        StructField('DestAirportCode', StringType()),\n        StructField('DepDel15', IntegerType()),\n        StructField('WindSpeed', DoubleType()),\n        StructField('SeaLevelPressure', DoubleType()),  \n        StructField('HourlyPrecip', DoubleType())])"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Create a new DataFrame from the CSV files, applying the schema"],"metadata":{}},{"cell_type":"code","source":["dfDelays = spark.read.csv(\"wasbs://\" + containerName + \"@\" + accountName + \".blob.core.windows.net/FlightsAndWeather/*/*/FlightsAndWeather.csv\",\n                    schema=data_schema,\n                    sep=\",\",\n                    header=True)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Load the trained machine learning model you created earlier in the lab"],"metadata":{}},{"cell_type":"code","source":["# Load the saved pipeline model\nmodel = PipelineModel.load(\"/flightDelayModel\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Make a prediction against the loaded data set"],"metadata":{}},{"cell_type":"code","source":["# Make a prediction against the dataset\nprediction = model.transform(dfDelays)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Save the scored data into a new global table called **scoredflights**"],"metadata":{}},{"cell_type":"code","source":["prediction.write.mode(\"overwrite\").saveAsTable(\"scoredflights\")"],"metadata":{},"outputs":[],"execution_count":15}],"metadata":{"name":"04 Deploy for Batch Scoring","notebookId":445132884519752},"nbformat":4,"nbformat_minor":0}